# ğŸ§  Laboratorio 3 â€“ Tracking de Modelos de Lenguaje (LLMs) con MLflow

## ğŸ¯ Objetivo
Aplicar **MLflow** para rastrear, comparar y registrar ejecuciones de modelos de lenguaje, analizando sus mÃ©tricas y comportamiento en diferentes proveedores (Gemini y Ollama).

---

## âš™ï¸ Parte 3 â€“ Registro y Tracking de LLMs

Se realizaron dos ejecuciones:

1. **Gemini (Google API)**
2. **Ollama (modelo local Llama3.1)**

Durante cada ejecuciÃ³n se registraron:

- **ParÃ¡metros:** modelo, temperatura, proveedor y tipo de tarea.  
- **MÃ©tricas:** latencia, total de tokens y costo estimado.  
- **Artefactos:** prompts y respuestas generadas.

Ambos modelos se registraron en el **Model Registry** como:

- `llm_gemini_chat`
- `llm_ollama_chat`

Luego se promovieron a los estados **Staging** y **Production**.

---

## ğŸ“Š Parte 4 â€“ AnÃ¡lisis comparativo

En la interfaz de MLflow, se compararon las mÃ©tricas de ambos modelos.

| Modelo | Proveedor | Temperatura | Latencia (s) | Total Tokens | Costo Estimado | Tipo de tarea |
|:--------|:-----------|:-------------|:--------------|:---------------|:----------------|:----------------|
| **Gemini** | Google | 0.7 | 10.61 | 1289 | 0.0019335 | Chat |
| **Ollama** | Local (Llama3.1) | 0.6 | 2.04 | 10 | 0.000001 | Chat |

---

## ğŸ§© InterpretaciÃ³n de resultados

- ğŸ•“ **Latencia promedio:**  
  El modelo **Ollama (Llama3.1)** tuvo **menor latencia (â‰ˆ2.0 s)** frente a **Gemini (â‰ˆ10.6 s)**, debido a que corre localmente sin depender de llamadas HTTP externas.

- ğŸ’¬ **Cantidad de tokens:**  
  **Gemini** generÃ³ **mÃ¡s tokens (1289)**, reflejando una respuesta mÃ¡s extensa y detallada.

- ğŸ’¸ **Costo estimado:**  
  El costo simulado de Gemini fue mayor, ya que su generaciÃ³n fue mÃ¡s larga y depende de un servicio remoto.

---

## âš™ï¸ ParÃ¡metros a optimizar en futuros experimentos

- **Temperatura:** ajustar entre 0.4â€“0.8 para equilibrar creatividad y coherencia.
- **Modelo:** probar versiones mÃ¡s ligeras o rÃ¡pidas (por ejemplo `gemini-1.5-flash` o `llama3.2`).
- **TamaÃ±o del prompt:** reducir longitud para menor latencia y costo.
- **Batching / caching local:** para mejorar tiempos en mÃºltiples inferencias.

---

## âœ… ConclusiÃ³n

El uso de **MLflow** permitiÃ³ registrar y comparar fÃ¡cilmente distintos tipos de modelos (tradicionales y LLMs), visualizar sus mÃ©tricas en la interfaz y gestionar versiones en el **Model Registry**, demostrando la utilidad del tracking para experimentaciÃ³n reproducible y anÃ¡lisis comparativo entre proveedores.
